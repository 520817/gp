{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/HAB/W0rNLf9bGQtG3Jct",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/520817/gp/blob/main/adhd_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3wuU86tE65c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "np.random.seed(42)\n",
        "cudnn.benchmark = False\n",
        "cudnn.deterministic = True\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "hHiShMlYFe58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3IXkvipR0mh",
        "outputId": "098a8bba-5e45-4943-ac37-52150599e57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "PL2JL6a_FfuX",
        "outputId": "719c00a0-f500-4f80-8333-1e31726d2c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f9c260299c38>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \"\"\"\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[0;32m--> 453\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kmounlp/NER.git"
      ],
      "metadata": {
        "id": "MSGy3bujF0I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob"
      ],
      "metadata": {
        "id": "x-T8lhgCNs-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list=[]"
      ],
      "metadata": {
        "id": "NglrP3LLNu1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in os.walk('NER/'):\n",
        "  for y in glob.glob(os.path.join(x[0], '*_NER.txt')):\n",
        "    file_list.append(y)"
      ],
      "metadata": {
        "id": "CO6Qof1eNxLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = sorted(file_list)"
      ],
      "metadata": {
        "id": "e9mgRtR0N6CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "file_path = file_list[0]\n",
        "file_path = Path(file_path)\n",
        "raw_text = file_path.read_text().strip()"
      ],
      "metadata": {
        "id": "ATeQEDq5OB0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install korpora\n",
        "\n",
        "from Korpora import Korpora\n",
        "corpus = Korpora.load(\"naver_changwon_ner\")"
      ],
      "metadata": {
        "id": "5rXoICJNO83V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "네이버 데이터셋 전처리"
      ],
      "metadata": {
        "id": "9k8XkJDkrT3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naver_read_file(file_list):\n",
        "\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "\n",
        "    for doc in file_list:\n",
        "        tokens = []\n",
        "        tags = []\n",
        "        list1=doc.words\n",
        "        list2=doc.tags\n",
        "\n",
        "        for text,tag in zip(list1,list2):\n",
        "            for i, syllable in enumerate(text): # 음절 단위로 자르고\n",
        "                tokens.append(syllable)\n",
        "                if tag == '-':\n",
        "                  tag = 'O' # 태그가 '-'인 경우 'O'로 변경\n",
        "                tags.append(tag)\n",
        "\n",
        "        token_docs.append(tokens)\n",
        "        tag_docs.append(tags)\n",
        "\n",
        "    return token_docs, tag_docs"
      ],
      "metadata": {
        "id": "0jp7_bpxPYvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naver_text,naver_tags =naver_read_file(corpus.train)"
      ],
      "metadata": {
        "id": "THjYXlx8e024"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(naver_text))\n",
        "print(len(naver_tags))"
      ],
      "metadata": {
        "id": "5V1r7hKMe06z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(naver_text[0], end='\\n\\n') # 음절 단위로 잘 잘렸네요!\n",
        "print(naver_tags[0])"
      ],
      "metadata": {
        "id": "q9-iJwb1nNNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "해양 뭐시기 데이터셋 전처리"
      ],
      "metadata": {
        "id": "_4L-7mfvrZEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def read_file(file_list):\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "    for file_path in file_list:\n",
        "        #print(\"read file from \", file_path)\n",
        "        file_path = Path(file_path)\n",
        "        raw_text = file_path.read_text().strip()\n",
        "        raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
        "\n",
        "        for doc in raw_docs:\n",
        "            tokens = []\n",
        "            tags = []\n",
        "            for line in doc.split('\\n'):\n",
        "                if line[0:1] == \"$\" or line[0:1] == \";\" or line[0:2] == \"##\":\n",
        "                    continue\n",
        "                token = line.split('\\t')[0]\n",
        "                tag = line.split('\\t')[3]\n",
        "                for i, syllable in enumerate(token):    # 음절 단위로 잘라서\n",
        "                    tokens.append(syllable)\n",
        "                    tags.append(tag)\n",
        "\n",
        "            token_docs.append(tokens)\n",
        "            tag_docs.append(tags)\n",
        "\n",
        "    return token_docs, tag_docs"
      ],
      "metadata": {
        "id": "xOSLg1QNrfvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts, tags = read_file(file_list[:])"
      ],
      "metadata": {
        "id": "o724eldkrNL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts))\n",
        "print(len(tags))"
      ],
      "metadata": {
        "id": "fA1kWKcKtcCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[0], end='\\n\\n') # 음절 단위로 잘 잘렸네요!\n",
        "print(tags[0])"
      ],
      "metadata": {
        "id": "VDlObStTtcMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "분리된 토큰을 하나의 개체명으로 묶어 주는 태깅 시스템을 위해 있는게 BIO, BIESO임.\n",
        "\n",
        "여러 개의 토큰으로 이루어진 개체명의 경우\n",
        "\n",
        "**개체명이 시작할 때** 'B,begin'\n",
        "\n",
        "**토큰이 개체명 중간에 있을 때** 'I,inside'\n",
        "\n",
        "**개체명의 마지막에 위치할 때** 'E,end'\n",
        "\n",
        "**하나의 토큰이 곧 하나의 개체명일 때** 'S,singleton'\n",
        "\n",
        "**토큰이 개체명이 아닐 경우** 'O,outside'"
      ],
      "metadata": {
        "id": "8w7_-eiNbz5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}\n",
        "\n",
        "for i, tag in enumerate(unique_tags):\n",
        "    print(tag)  #해양 tags 변경 전"
      ],
      "metadata": {
        "id": "SF0eRYxDkqwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sublist in tags:\n",
        "    for i, label in enumerate(sublist):\n",
        "        if label == 'I-NOH':\n",
        "            sublist[i] = 'NOH_I'\n",
        "        elif label == 'I-PNT':\n",
        "            sublist[i] = 'PNT_I'\n",
        "        elif label == 'I-ORG':\n",
        "            sublist[i] = 'ORG_I'\n",
        "        elif label == 'I-DUR':\n",
        "            sublist[i] = 'DUR_I'\n",
        "        elif label == 'I-TIM':\n",
        "            sublist[i] = 'TIM_I'\n",
        "        elif label == 'I-PER':\n",
        "            sublist[i] = 'PER_I'\n",
        "        elif label == 'I-MNY':\n",
        "            sublist[i] = 'MNY_I'\n",
        "        elif label == 'B-MNY':\n",
        "            sublist[i] = 'MNY_B'\n",
        "        elif label == 'B-DAY':\n",
        "            sublist[i] = 'DAY_B'\n",
        "        elif label == 'B-TIM':\n",
        "            sublist[i] = 'TIM_B'\n",
        "        elif label == 'I-LOC':\n",
        "            sublist[i] = 'LOC_I'\n",
        "        elif label == 'B-PER':\n",
        "            sublist[i] = 'PER_B'\n",
        "        elif label == 'B-ORG':\n",
        "            sublist[i] = 'ORG_B'\n",
        "        elif label == 'B-LOC':\n",
        "            sublist[i] = 'LOC_B'\n",
        "        elif label == 'B-PNT':\n",
        "            sublist[i] = 'PNT_B'\n",
        "        elif label == 'B-NOH':\n",
        "            sublist[i] = 'NOH_B'\n",
        "        elif label == 'I-PER':\n",
        "            sublist[i] = 'PER_I'\n",
        "        elif label == 'B-LOC':\n",
        "            sublist[i] = 'LOC_B'\n",
        "        elif label == 'I-POH':\n",
        "            sublist[i] = 'POH_I'\n",
        "        elif label == 'I-DAT':\n",
        "            sublist[i] = 'DAT_I'\n",
        "        elif label == 'B-POH':\n",
        "            sublist[i] = 'POH_B'\n",
        "        elif label == 'B-DUR':\n",
        "            sublist[i] = 'DUR_B'\n",
        "        elif label == 'B-DAT':\n",
        "            sublist[i] = 'DAT_B'\n",
        "        elif label == '0':\n",
        "            sublist[i] = '0'"
      ],
      "metadata": {
        "id": "pw1se6PFj5sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}\n",
        "\n",
        "for i, tag in enumerate(unique_tags):\n",
        "    print(tag)  #해양 tags 변경 후"
      ],
      "metadata": {
        "id": "h7bFUznW56C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts.extend(naver_text)\n",
        "tags.extend(naver_tags)"
      ],
      "metadata": {
        "id": "0o94hYy66Fvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "metadata": {
        "id": "VHE_IE4O6jui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, tag in enumerate(unique_tags):\n",
        "    print(tag)  # 학습을 위한 label list를 확인합니다."
      ],
      "metadata": {
        "id": "zKpiYcZE6lW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tag in list(tag2id.keys()) :\n",
        "    globals()[tag] = 0\n",
        "\n",
        "for tag in tags :\n",
        "    for ner in tag :\n",
        "        globals()[ner] += 1\n",
        "\n",
        "for tag in list(tag2id.keys()) :\n",
        "    print('{:>6} : {:>7,}'. format(tag, globals()[tag]))"
      ],
      "metadata": {
        "id": "dZKSVM847DXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, test_texts, train_tags, test_tags = train_test_split(texts, tags, test_size=.2, random_state=42)"
      ],
      "metadata": {
        "id": "SDN7pFeX7ht2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train 문장 : {:>6,}' .format(len(train_texts)))\n",
        "print('Train 태그 : {:>6,}' .format(len(train_tags)))\n",
        "print('Test  문장 : {:>6,}' .format(len(test_texts)))\n",
        "print('Test  태그 : {:>6,}' .format(len(test_tags)))"
      ],
      "metadata": {
        "id": "q-YH9ZnT7hx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts[5]"
      ],
      "metadata": {
        "id": "TZOGxTHa7h1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT 토크나이저"
      ],
      "metadata": {
        "id": "rrTVAmV57umZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "MODEL_NAME = \"beomi/KcELECTRA-base-v2022\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "HWlZUwuB7h5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "metadata": {
        "id": "LgYgUvlX7z4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_token_id = tokenizer.pad_token_id # 0\n",
        "cls_token_id = tokenizer.cls_token_id # 101\n",
        "sep_token_id = tokenizer.sep_token_id # 102\n",
        "pad_token_label_id = tag2id['O']    # tag2id['O']\n",
        "cls_token_label_id = tag2id['O']\n",
        "sep_token_label_id = tag2id['O']"
      ],
      "metadata": {
        "id": "KyKDtxJK73Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_tokenizer(sent, max_seq_length):\n",
        "    pre_syllable = \"_\"\n",
        "    input_ids = [pad_token_id] * (max_seq_length - 1)\n",
        "    attention_mask = [0] * (max_seq_length - 1)\n",
        "    token_type_ids = [0] * max_seq_length\n",
        "    sent = sent[:max_seq_length-2]\n",
        "\n",
        "    for i, syllable in enumerate(sent):\n",
        "        if syllable == '_':\n",
        "            pre_syllable = syllable\n",
        "        if pre_syllable != \"_\":\n",
        "            syllable = '##' + syllable  # 중간 음절에는 모두 prefix를 붙입니다.\n",
        "            # 우리가 구성한 학습 데이터도 이렇게 구성되었기 때문이라고 함.\n",
        "            # 이순신은 조선 -> [이, ##순, ##신, ##은, 조, ##선]\n",
        "        pre_syllable = syllable\n",
        "\n",
        "        input_ids[i] = (tokenizer.convert_tokens_to_ids(syllable))\n",
        "        attention_mask[i] = 1\n",
        "\n",
        "    input_ids = [cls_token_id] + input_ids\n",
        "    input_ids[len(sent)+1] = sep_token_id\n",
        "    attention_mask = [1] + attention_mask\n",
        "    attention_mask[len(sent)+1] = 1\n",
        "    return {\"input_ids\":input_ids,\n",
        "            \"attention_mask\":attention_mask,\n",
        "            \"token_type_ids\":token_type_ids}"
      ],
      "metadata": {
        "id": "H56iODmK73I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ner_tokenizer(train_texts[0], 5))"
      ],
      "metadata": {
        "id": "uNzD1LHq78Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_sentences = []\n",
        "tokenized_test_sentences = []\n",
        "\n",
        "for text in train_texts:    # 전체 데이터를 tokenizing 합니다.\n",
        "    tokenized_train_sentences.append(ner_tokenizer(text, 128))\n",
        "for text in test_texts:\n",
        "    tokenized_test_sentences.append(ner_tokenizer(text, 128))"
      ],
      "metadata": {
        "id": "TvaNc5Nr78Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_tags(tags, max_seq_length):\n",
        "    # label 역시 입력 token과 개수를 맞춰줍니다\n",
        "    tags = tags[:max_seq_length-2]\n",
        "    labels = [tag2id[tag] for tag in tags]\n",
        "    labels = [tag2id['O']] + labels\n",
        "\n",
        "    padding_length = max_seq_length - len(labels)\n",
        "    labels = labels + ([pad_token_label_id] * padding_length)\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "VuyXg4Zo78bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2id"
      ],
      "metadata": {
        "id": "7jmQTT2X8B7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode_tags(train_tags[0], 5)"
      ],
      "metadata": {
        "id": "d2JC7k6e8CAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = []\n",
        "test_labels = []\n",
        "\n",
        "for tag in train_tags:\n",
        "    train_labels.append(encode_tags(tag, 128))\n",
        "\n",
        "for tag in test_tags:\n",
        "    test_labels.append(encode_tags(tag, 128))"
      ],
      "metadata": {
        "id": "G0u4iCxr8CEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_labels), len(test_labels)"
      ],
      "metadata": {
        "id": "PzyG7_6v8CJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token 데이터셋"
      ],
      "metadata": {
        "id": "UsmqYe7y8N92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class TokenDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = TokenDataset(tokenized_train_sentences, train_labels)\n",
        "test_dataset = TokenDataset(tokenized_test_sentences, test_labels)"
      ],
      "metadata": {
        "id": "9VO4H2NC8CNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "hVMx73Ty8nTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification, Trainer, TrainingArguments, AutoModelForTokenClassification,EarlyStoppingCallback\n",
        "import sys\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=5,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=1000, # 1000번쨰 steps마다 log를 보여줌\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=5,\n",
        "    save_strategy='steps', # steps로 해야 earlystop이 가능\n",
        "    evaluation_strategy='steps',\n",
        "    save_steps=1000, # 1000번쨰 step마다 저장\n",
        "    eval_steps=1000, # 1000번째 step마다 평가\n",
        "    seed=15,\n",
        "    load_best_model_at_end=True # 가장 좋은 성능의 모델로...\n",
        ")"
      ],
      "metadata": {
        "id": "tN6IMVxC8Q-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_R6K6En8RCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NIBjHBBG7z8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oftyZjZq7WTm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}