{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bd0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import re\n",
    "import pandas as pd\n",
    "from konlpy.tag import Komoran\n",
    "from dateutil import parser\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8896700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForTokenClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(54343, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=39, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장한 모델 불러오기\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "MODEL_NAME = \"beomi/KcELECTRA-base-v2022\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tag2id = {'POH_B': 0, 'CVL_B': 1, 'FLD_B': 2, 'FLD_I': 3, 'EVT_I': 4,'AFW_B': 5,'CVL_I': 6,'NUM_B': 7,'MAT_I': 8, 'TIM_I': 9,'ANM_I': 10,'PER_I': 11,'POH_I': 12,\n",
    " 'TRM_I': 13,\n",
    " 'TIM_B': 14,\n",
    " 'ANM_B': 15,\n",
    " 'O': 16,\n",
    " 'DAT_I': 17,\n",
    " 'DUR_I': 18,\n",
    " 'PNT_B': 19,\n",
    " 'PNT_I': 20,\n",
    " 'MNY_I': 21,\n",
    " 'EVT_B': 22,\n",
    " 'ORG_I': 23,\n",
    " 'MNY_B': 24,\n",
    " 'ORG_B': 25,\n",
    " 'LOC_B': 26,\n",
    " 'PLT_I': 27,\n",
    " 'MAT_B': 28,\n",
    " 'DAT_B': 29,\n",
    " 'NUM_I': 30,\n",
    " 'AFW_I': 31,\n",
    " 'PLT_B': 32,\n",
    " 'DUR_B': 33,\n",
    " 'PER_B': 34,\n",
    " 'LOC_I': 35,\n",
    " 'NOH_B': 36,\n",
    " 'TRM_B': 37,\n",
    " 'NOH_I': 38}\n",
    "unique_tags={'POH_B', 'CVL_B', 'FLD_B', 'FLD_I', 'EVT_I', 'AFW_B', 'CVL_I', 'NUM_B', 'MAT_I', 'TIM_I', 'ANM_I', 'PER_I', 'POH_I', 'TRM_I', 'TIM_B', 'ANM_B', 'O', 'DAT_I', 'DUR_I', 'PNT_B', 'PNT_I', 'MNY_I', 'EVT_B', 'ORG_I', 'MNY_B', 'ORG_B', 'LOC_B', 'PLT_I', 'MAT_B', 'DAT_B', 'NUM_I', 'AFW_I', 'PLT_B', 'DUR_B', 'PER_B', 'LOC_I', 'NOH_B', 'TRM_B', 'NOH_I'}\n",
    "id2tag={0: 'POH_B',\n",
    " 1: 'CVL_B',\n",
    " 2: 'FLD_B',\n",
    " 3: 'FLD_I',\n",
    " 4: 'EVT_I',\n",
    " 5: 'AFW_B',\n",
    " 6: 'CVL_I',\n",
    " 7: 'NUM_B',\n",
    " 8: 'MAT_I',\n",
    " 9: 'TIM_I',\n",
    " 10: 'ANM_I',\n",
    " 11: 'PER_I',\n",
    " 12: 'POH_I',\n",
    " 13: 'TRM_I',\n",
    " 14: 'TIM_B',\n",
    " 15: 'ANM_B',\n",
    " 16: 'O',\n",
    " 17: 'DAT_I',\n",
    " 18: 'DUR_I',\n",
    " 19: 'PNT_B',\n",
    " 20: 'PNT_I',\n",
    " 21: 'MNY_I',\n",
    " 22: 'EVT_B',\n",
    " 23: 'ORG_I',\n",
    " 24: 'MNY_B',\n",
    " 25: 'ORG_B',\n",
    " 26: 'LOC_B',\n",
    " 27: 'PLT_I',\n",
    " 28: 'MAT_B',\n",
    " 29: 'DAT_B',\n",
    " 30: 'NUM_I',\n",
    " 31: 'AFW_I',\n",
    " 32: 'PLT_B',\n",
    " 33: 'DUR_B',\n",
    " 34: 'PER_B',\n",
    " 35: 'LOC_I',\n",
    " 36: 'NOH_B',\n",
    " 37: 'TRM_B',\n",
    " 38: 'NOH_I'}\n",
    "pad_token_id = tokenizer.pad_token_id # 0\n",
    "cls_token_id = tokenizer.cls_token_id # 101\n",
    "sep_token_id = tokenizer.sep_token_id # 102\n",
    "pad_token_label_id = tag2id['O']    # tag2id['O']\n",
    "cls_token_label_id = tag2id['O']\n",
    "sep_token_label_id = tag2id['O']\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('saving_folder', num_labels=len(unique_tags))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "564645a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 토크나이저는 wordPiece tokenizer로 tokenizing 결과를 반환합니다.\n",
    "# 데이터 단위를 음절 단위로 변경했기 때문에, tokenizer도 음절 tokenizer로 변경\n",
    "\n",
    "# berttokenizer를 사용하는데 한국어 vocab이 8000개 정도 밖에 없고 그 안의 한국어들의 거의 음절로 존재\n",
    "# -> 음절 단위 tokenizer를 적용하면 vocab id를 어느 정도 획득할 수 있어 UNK가 별로 없을듯 하다\n",
    "def ner_tokenizer(sent, max_seq_length):    \n",
    "    pre_syllable = \"_\"\n",
    "    input_ids = [pad_token_id] * (max_seq_length - 1)\n",
    "    attention_mask = [0] * (max_seq_length - 1)\n",
    "    token_type_ids = [0] * max_seq_length\n",
    "    sent = sent[:max_seq_length-2]\n",
    "\n",
    "    for i, syllable in enumerate(sent):\n",
    "        if syllable == '_':\n",
    "            pre_syllable = syllable\n",
    "        if pre_syllable != \"_\":\n",
    "            syllable = '##' + syllable  # 중간 음절에는 모두 prefix를 붙입니다.\n",
    "            # 우리가 구성한 학습 데이터도 이렇게 구성되었기 때문이라고 함.\n",
    "            # 이순신은 조선 -> [이, ##순, ##신, ##은, 조, ##선]\n",
    "        pre_syllable = syllable\n",
    "\n",
    "        input_ids[i] = (tokenizer.convert_tokens_to_ids(syllable))\n",
    "        attention_mask[i] = 1\n",
    "    \n",
    "    input_ids = [cls_token_id] + input_ids\n",
    "    input_ids[len(sent)+1] = sep_token_id\n",
    "    attention_mask = [1] + attention_mask\n",
    "    attention_mask[len(sent)+1] = 1\n",
    "    return {\"input_ids\":input_ids,\n",
    "            \"attention_mask\":attention_mask,\n",
    "            \"token_type_ids\":token_type_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be17826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_inference(text) :\n",
    "\n",
    "    model.eval()\n",
    "    text = text.replace(' ', '/')\n",
    "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', text)\n",
    "\n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    tokenized_sent = ner_tokenizer(text, len(text)+2)\n",
    "    input_ids = torch.tensor(tokenized_sent['input_ids']).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(tokenized_sent['attention_mask']).unsqueeze(0).to(device)\n",
    "    token_type_ids = torch.tensor(tokenized_sent['token_type_ids']).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "\n",
    "    logits = outputs['logits']\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = token_type_ids.cpu().numpy()\n",
    "\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "    pred_tags = [list(tag2id.keys())[p_i] for p in predictions for p_i in p]\n",
    "\n",
    "    print('{}\\t{}'.format(\"TOKEN\", \"TAG\"))\n",
    "    print(\"===========\")\n",
    "    # for token, tag in zip(tokenizer.decode(tokenized_sent['input_ids']), pred_tags):\n",
    "    #   print(\"{:^5}\\t{:^5}\".format(token, tag))\n",
    "    for i, tag in enumerate(pred_tags):\n",
    "        print(\"{:^5}\\t{:^5}\".format(tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'][i]), tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7926cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\tTAG\n",
      "===========\n",
      "[CLS]\t  O  \n",
      "  2  \tNUM_B\n",
      " ##4 \tNUM_B\n",
      " ##0 \tNUM_B\n",
      " ##5 \tNUM_B\n",
      " ##1 \tNUM_B\n",
      " ##7 \tNUM_B\n",
      " ##저 \tNUM_B\n",
      " ##녁 \tTIM_B\n",
      " ##예 \t  O  \n",
      " ##약 \t  O  \n",
      "[SEP]\t  O  \n"
     ]
    }
   ],
   "source": [
    "text = '240517 저녁예약'\n",
    "ner_inference(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd9f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_inference2(text) : \n",
    "  \n",
    "    model.eval()\n",
    "    text = text.replace(' ', '/')\n",
    "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', text)\n",
    "\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    tokenized_sent = ner_tokenizer(text, len(text)+2)\n",
    "    input_ids = torch.tensor(tokenized_sent['input_ids']).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(tokenized_sent['attention_mask']).unsqueeze(0).to(device)\n",
    "    token_type_ids = torch.tensor(tokenized_sent['token_type_ids']).unsqueeze(0).to(device)    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "        \n",
    "    logits = outputs['logits']\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = token_type_ids.cpu().numpy()\n",
    "\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "    pred_tags = [list(tag2id.keys())[p_i] for p in predictions for p_i in p]\n",
    "\n",
    "    #print('{}\\t{}'.format(\"TOKEN\", \"TAG\"))\n",
    "    #for i, tag in enumerate(pred_tags):\n",
    "        #print(\"{:^5}\\t{:^5}\".format(tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'][i]), tag))\n",
    "    \n",
    "    #token_list = [tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'][i]) for i in range(len(tokenized_sent['input_ids']))]\n",
    "    #token_list = [re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', token) for token in token_list]\n",
    "    #token_tag_pairs = [(token_list[i], pred_tags[i]) for i in range(len(token_list))]\n",
    "    \n",
    "    #for pair in token_tag_pairs:\n",
    "        #print(pair)\n",
    "    \n",
    "    token_list = tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'])\n",
    "    token_list = [re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', token) for token in token_list]\n",
    "    token_tag_pairs = []\n",
    "    current_tag = None\n",
    "    current_token = ''\n",
    "    for token, tag in zip(token_list, pred_tags):\n",
    "        if tag == current_tag:\n",
    "            current_token += token\n",
    "        else:\n",
    "            if current_token:\n",
    "                token_tag_pairs.append((current_token, current_tag))\n",
    "            current_token = token\n",
    "            current_tag = tag\n",
    "    if current_token:\n",
    "        token_tag_pairs.append((current_token, current_tag))\n",
    "\n",
    "    return token_tag_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24508d90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CLS', 'O'),\n",
       " ('오늘', 'DAT_B'),\n",
       " ('밤', 'TIM_B'),\n",
       " ('8시부터9시까지', 'TIM_I'),\n",
       " ('세미나SEP', 'O')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '오늘 밤 8시부터 9시까지 세미나'\n",
    "ner_inference2(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c5af9",
   "metadata": {},
   "source": [
    "## 최종 ner모델 = ner_inference3함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe071b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_inference3(text):\n",
    "    model.eval()\n",
    "    text = text.replace(' ', '/')\n",
    "    text = re.sub('[-=+#/\\?:^$.@*\\\"※&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', text)\n",
    "\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    tokenized_sent = ner_tokenizer(text, len(text) + 2)\n",
    "    input_ids = torch.tensor(tokenized_sent['input_ids']).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(tokenized_sent['attention_mask']).unsqueeze(0).to(device)\n",
    "    token_type_ids = torch.tensor(tokenized_sent['token_type_ids']).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "\n",
    "    logits = outputs['logits']\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = token_type_ids.cpu().numpy()\n",
    "\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "    pred_tags = [list(tag2id.keys())[p_i] for p in predictions for p_i in p]\n",
    "\n",
    "    token_list = tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'])\n",
    "    token_list = [re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', token) for token in token_list]\n",
    "\n",
    "    token_tag_pairs = []\n",
    "    rest= []\n",
    "    current_tag = None\n",
    "    current_token = ''\n",
    "    for token, tag in zip(token_list, pred_tags):\n",
    "        if tag in ['NUM_B','NUM_I','DUR_B','DUR_I', 'TIM_B','TIM_I', 'DAT_B','DAT_I']:\n",
    "            token_tag_pairs.append(token)\n",
    "            tokenized_text = ''.join(token_tag_pairs)\n",
    "        else:\n",
    "            rest.append(token)\n",
    "        \n",
    "    rest = rest[1:-1]\n",
    "    rest = ''.join(rest)\n",
    "            \n",
    "    return tokenized_text, rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e238005c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('오늘밤8시부터9시까지', '세미나')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '오늘 밤 8시부터 9시까지 세미나'\n",
    "ner_inference3(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4033e3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5월14일부터5월19일까지', '부산여행간다')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '5월14일부터 5월19일까지 부산여행 간다'\n",
    "ner_inference3(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8b76c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('20240516부터20240518까지', '강원도강릉출장있음ㅜㅜ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '20240516부터 20240518까지 강원도 강릉 출장있음 ㅜㅜ'\n",
    "ner_inference3(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ca0c5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3일뒤에', '경복궁관련발표있음')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_inference3('3일뒤에경복궁 관련 발표있음')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df41a4e",
   "metadata": {},
   "source": [
    "### 이제 라벨 11개 나오게 해야지..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e734694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_schedule_info(sentence,event):\n",
    "    if '부터' in sentence: #부터만 쓰고 까지를 쓰지 않는 경우 종료기간 정규표현식에 들어갈 수 있게끔\n",
    "         sentence = sentence + \"까지\"\n",
    "    if ('부터' in sentence) and ('에' in sentence):\n",
    "        sentence = sentence.replace('에','까지')\n",
    "    if 'UNK' in sentence: #특수기호로 기간을 나타내는 경우 문자로 대체해서 정규표현식에 들어갈 수 있게끔\n",
    "        sentence=sentence.replace(\"UNK\", \"부터\")\n",
    "        sentence = sentence + \"까지\"\n",
    "    days = ['월요일', '화요일', '수요일', '목요일', '금요일', '토요일', '일요일']\n",
    "    weeks = ['저번주', '이번주', '다음주', '다다음주', '다다다음주']\n",
    "\n",
    "    # 조건 확인\n",
    "    if any(day in sentence for day in days) and not any(week in sentence for week in weeks):\n",
    "        sentence = \"이번주 \" + sentence\n",
    "        \n",
    "    \n",
    "    \n",
    "    text = re.sub('[-=+,#/\\?:^$.@*\\\"~※&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', sentence)  # 특수문자 제거\n",
    "    text = text.replace(\" \", \"\")  # 모든 공백 제거\n",
    "    \n",
    "    # 시작 기간을 추출하는 정규표현식\n",
    "    start_time_pattern = r'(?:(그제|어제|오늘|낼|내일|모래|모레|이번주|다음주|다다음주|\\d+년|\\d{8}|\\d{7}|\\d{6}|\\d{5}|\\d{4}))?(?:(월요일|화요일|수요일|목요일|금요일|토요일|일요일|월|화|수|목|금|토|일))?(?:(\\d+)월)?(?:(\\d+)일)?(?:(오전|오후|새벽|밤|낮|저녁|아침))?(?:(\\d+)시)?(?:(반|\\d+분))?(?:부터)?' # 시작 기간 추출\n",
    "    match = re.search(start_time_pattern, text)\n",
    "    \n",
    "        \n",
    "    if match:\n",
    "        syear = match.group(1) if match.group(1) else datetime.now().year  # 년도를 추출합니다. 년도가 없으면 기본값으로 2024를 사용합니다.\n",
    "        smonth = match.group(3).strip(\"월\") if match.group(3) else datetime.now().month\n",
    "        sday = match.group(4).strip(\"일\") if match.group(4) else datetime.now().day\n",
    "        sdow = match.group(2) \n",
    "        \n",
    "        if match.group(1) and match.group(1).isdigit():\n",
    "            if len(match.group(1)) == 8:  # yyyyddmm 형식\n",
    "                syear = int(match.group(1)[:4])\n",
    "                smonth = int(match.group(1)[4:6])\n",
    "                sday = int(match.group(1)[6:])\n",
    "            elif len(match.group(1)) == 6:  # yyddmm 형식\n",
    "                syear = int(match.group(1)[:2]) + 2000\n",
    "                smonth = int(match.group(1)[2:4])\n",
    "                sday = int(match.group(1)[4:])\n",
    "            elif len(match.group(1)) == 5:  # yydm 형식\n",
    "                syear = int(match.group(1)[:2]) + 2000\n",
    "                sday_month = int(match.group(1)[2:])\n",
    "                smonth = sday_month // 100\n",
    "                sday = sday_month % 100\n",
    "            elif len(match.group(1)) == 7:  # yyyydm 형식\n",
    "                syear = int(match.group(1)[:4])\n",
    "                sday_month = int(match.group(1)[4:])\n",
    "                smonth = sday_month // 100\n",
    "                sday = sday_month % 100\n",
    "            elif len(match.group(1)) == 4:\n",
    "                syear = datetime.now().year\n",
    "                smonth=int(match.group()[:2])\n",
    "                sday=int(match.group(1)[2:])\n",
    "\n",
    "        # 유효한 날짜인지 확인\n",
    "            if not (1 <= smonth <= 12 and 1 <= sday <= 31):\n",
    "                smonth = None\n",
    "                sday = None\n",
    "        \n",
    "        \n",
    "        elif match.group(1) == '오늘':\n",
    "            syear = datetime.now().year\n",
    "            smonth = datetime.now().month\n",
    "            sday = datetime.now().day\n",
    "        elif match.group(1) == '어제':\n",
    "            date = datetime.now() - timedelta(days=1)\n",
    "            syear = date.year\n",
    "            smonth = date.month\n",
    "            sday = date.day\n",
    "        elif (match.group(1) == '내일') or (match.group(1) == '낼') :\n",
    "            date = datetime.now() + timedelta(days=1)\n",
    "            syear = date.year\n",
    "            smonth = date.month\n",
    "            sday = date.day\n",
    "        elif (match.group(1) == '모레') or (match.group(1) == '모래'):\n",
    "            date = datetime.now() + timedelta(days=2)\n",
    "            syear = date.year\n",
    "            smonth = date.month\n",
    "            sday = date.day\n",
    "        elif match.group(1) == '그제':\n",
    "            date = datetime.now() - timedelta(days=2)\n",
    "            syear = date.year\n",
    "            smonth = date.month\n",
    "            sday = date.day\n",
    "\n",
    "            \n",
    "        elif match.group(1) == '다음주':\n",
    "            if match.group(2):  # 요일이 주어진 경우\n",
    "                today = datetime.now().date()\n",
    "                weekday_str = match.group(2)\n",
    "                weekdays = {\n",
    "                    '월요일': 0,\n",
    "                    '화요일': 1,\n",
    "                    '수요일': 2,\n",
    "                    '목요일': 3,\n",
    "                    '금요일': 4,\n",
    "                    '토요일': 5,\n",
    "                    '일요일': 6\n",
    "                }\n",
    "                target_weekday = weekdays[weekday_str]\n",
    "                current_weekday = today.weekday()\n",
    "                days_to_target = target_weekday - current_weekday\n",
    "                #if days_to_target >=0:  # 이미 해당 요일이 지난 경우\n",
    "                days_to_target += 7\n",
    "                date = datetime.now() + timedelta(days=days_to_target)\n",
    "                syear = date.year\n",
    "                smonth = date.month\n",
    "                sday = date.day\n",
    "                \n",
    "                    \n",
    "            else:  # 요일이 주어지지 않은 경우\n",
    "                date = datetime.now() + timedelta(days=7)\n",
    "                syear = date.year\n",
    "                smonth = date.month\n",
    "                sday = date.day\n",
    "\n",
    "            \n",
    "        elif match.group(1) == '다다음주':\n",
    "             if match.group(2):  # 요일이 주어진 경우\n",
    "                today = datetime.now().date()\n",
    "                weekday_str = match.group(2)\n",
    "                weekdays = {\n",
    "                    '월요일': 0,\n",
    "                    '화요일': 1,\n",
    "                    '수요일': 2,\n",
    "                    '목요일': 3,\n",
    "                    '금요일': 4,\n",
    "                    '토요일': 5,\n",
    "                    '일요일': 6\n",
    "                }\n",
    "                target_weekday = weekdays[weekday_str]\n",
    "                current_weekday = today.weekday()\n",
    "                days_to_target = target_weekday - current_weekday\n",
    "                #if days_to_target >=0:  # 이미 해당 요일이 지난 경우\n",
    "                days_to_target += 14\n",
    "                date = datetime.now() + timedelta(days=days_to_target)\n",
    "                syear = date.year\n",
    "                smonth = date.month\n",
    "                sday = date.day\n",
    "                \n",
    "            \n",
    "             else:\n",
    "                date = datetime.now() + timedelta(days=14)\n",
    "                syear = date.year\n",
    "                smonth = date.month\n",
    "                sday = date.day\n",
    "            \n",
    "            \n",
    "        elif match.group(1) == '이번주':\n",
    "            today = datetime.now().date()\n",
    "            weekday_str = match.group(2)\n",
    "            weekdays = {\n",
    "                '월요일': 0,\n",
    "                '화요일': 1,\n",
    "                '수요일': 2,\n",
    "                '목요일': 3,\n",
    "                '금요일': 4,\n",
    "                '토요일': 5,\n",
    "                '일요일': 6\n",
    "            }\n",
    "            target_weekday = weekdays[weekday_str]\n",
    "            current_weekday = today.weekday()\n",
    "            days_to_target = (target_weekday - current_weekday) % 7\n",
    "            date = datetime.now() + timedelta(days=days_to_target)\n",
    "            syear = date.year\n",
    "            smonth = date.month\n",
    "            sday = date.day\n",
    "        elif match.group(1) and '년' in match.group(1):\n",
    "            syear = int(match.group(1).strip(\"년\"))\n",
    "        else:\n",
    "            syear = int(datetime.now().year)\n",
    "    \n",
    "            \n",
    "        stime_period = match.group(5)\n",
    "        shour = int(match.group(6).strip(\"시\")) if match.group(6) else 0\n",
    "        \n",
    "        \n",
    "        if (stime_period == '오후' or stime_period == '저녁' or stime_period == '밤' or stime_period == '낮') and int(shour) < 12:\n",
    "            shour = int(shour) + 12\n",
    "        if match.group(7) and '반' in match.group(7):\n",
    "            sminute = 30\n",
    "        elif match.group(7):\n",
    "            sminute = int(match.group(7).strip(\"분\"))\n",
    "        else:\n",
    "            sminute = 0\n",
    "        \n",
    "        # 종료 기간을 추출하는 정규표현식\n",
    "        end_time_pattern = r'(?:(어제|오늘|내일|모레|다음주|다다음주|\\d+년|\\d{8}|\\d{7}|\\d{6}|\\d{5}|\\d{4}))?(?:(월|화|수|목|금|토|일|월요일|화요일|수요일|목요일|금요일|토요일|일요일))?(?:(\\d+)월)?(?:(\\d+)일)?(?:(오전|오후|새벽|밤|낮|저녁|아침))?(?:(\\d+)시)?(?:(반|\\d+분))?(까지)' # 종료 기간 추출\n",
    "        end_match = re.search(end_time_pattern, text)\n",
    "        if end_match:\n",
    "            fyear = end_match.group(1) if end_match.group(1) and end_match.group(1).isdigit() else syear  # 종료 년도를 추출합니다.\n",
    "            fmonth = end_match.group(3).strip(\"월\") if end_match.group(3) else smonth\n",
    "            fday = end_match.group(4).strip(\"일\") if end_match.group(4) else sday\n",
    "            ftime_period = end_match.group(5)\n",
    "            fhour = int(end_match.group(6).strip(\"시\")) if end_match.group(6) else 0\n",
    "            \n",
    "            if end_match.group(1) and end_match.group(1).isdigit():\n",
    "                if len(end_match.group(1)) == 8:  # yyyyddmm 형식\n",
    "                    fyear = int(end_match.group(1)[:4])\n",
    "                    fmonth = int(end_match.group(1)[4:6])\n",
    "                    fday = int(end_match.group(1)[6:])\n",
    "                elif len(end_match.group(1)) == 6:  # yyddmm 형식\n",
    "                    fyear = int(end_match.group(1)[:2]) + 2000\n",
    "                    fmonth = int(end_match.group(1)[2:4])\n",
    "                    fday = int(end_match.group(1)[4:])\n",
    "                elif len(end_match.group(1)) == 5:  # yydm 형식\n",
    "                    fyear = int(end_match.group(1)[:2]) + 2000\n",
    "                    fday_month = int(end_match.group(1)[2:])\n",
    "                    fmonth = fday_month // 100\n",
    "                    fday = fday_month % 100\n",
    "                elif len(end_match.group(1)) == 7:  # yyyydm 형식\n",
    "                    fyear = int(end_match.group(1)[:4])\n",
    "                    fday_month = int(end_match.group(1)[4:])\n",
    "                    fmonth = fday_month // 100\n",
    "                    fday = fday_month % 100\n",
    "                elif len(end_match.group(1)) == 4:\n",
    "                    fyear = datetime.now().year\n",
    "                    fmonth=int(end_match.group()[:2])\n",
    "                    fday=int(end_match.group(1)[2:])\n",
    "\n",
    "            # 유효한 날짜인지 확인\n",
    "                if not (1 <= fmonth <= 12 and 1 <= fday <= 31):\n",
    "                    fmonth = None\n",
    "                    fday = None\n",
    "            \n",
    "            \n",
    "            elif end_match.group(1) == '다음주':\n",
    "                if end_match.group(2):  # 요일이 주어진 경우\n",
    "                    today = datetime.now().date()\n",
    "                    weekday_str2 = end_match.group(2)\n",
    "                    weekdays = {\n",
    "                    '월요일': 0,\n",
    "                    '화요일': 1,\n",
    "                    '수요일': 2,\n",
    "                    '목요일': 3,\n",
    "                    '금요일': 4,\n",
    "                    '토요일': 5,\n",
    "                    '일요일': 6\n",
    "                    }\n",
    "                    target_weekday2 = weekdays[weekday_str2]\n",
    "                    current_weekday2 = today.weekday()\n",
    "                    days_to_target2 = target_weekday2 - current_weekday2\n",
    "                    #if days_to_target >=0:  # 이미 해당 요일이 지난 경우\n",
    "                    days_to_target2 += 7\n",
    "                    date = datetime.now() + timedelta(days=days_to_target2)\n",
    "                    fyear = date.year\n",
    "                    fmonth = date.month\n",
    "                    fday = date.day\n",
    "                    \n",
    "                else:  # 요일이 주어지지 않은 경우\n",
    "                    date = datetime.now() + timedelta(days=7)\n",
    "                    fyear = date.year\n",
    "                    fmonth = date.month\n",
    "                    fday = date.day\n",
    "\n",
    "            elif end_match.group(1) == '다다음주':\n",
    "                if end_match.group(2):  # 요일이 주어진 경우\n",
    "                    today = datetime.now().date()\n",
    "                    weekday_str2 = end_match.group(2)\n",
    "                    weekdays = {\n",
    "                    '월요일': 0,\n",
    "                    '화요일': 1,\n",
    "                    '수요일': 2,\n",
    "                    '목요일': 3,\n",
    "                    '금요일': 4,\n",
    "                    '토요일': 5,\n",
    "                    '일요일': 6\n",
    "                    }\n",
    "                    target_weekday2 = weekdays[weekday_str2]\n",
    "                    current_weekday2 = today.weekday()\n",
    "                    days_to_target2 = target_weekday2 - current_weekday2\n",
    "                    #if days_to_target >=0:  # 이미 해당 요일이 지난 경우\n",
    "                    days_to_target2 += 14\n",
    "                    date = datetime.now() + timedelta(days=days_to_target2)\n",
    "                    fyear = date.year\n",
    "                    fmonth = date.month\n",
    "                    fday = date.day\n",
    "                \n",
    "                else:\n",
    "                    date = datetime.now() + timedelta(days=14)\n",
    "                    fyear = date.year\n",
    "                    fmonth = date.month\n",
    "                    fday = date.day\n",
    "                \n",
    "            elif end_match.group(1) == '이번주':\n",
    "                today = datetime.now().date()\n",
    "                weekday_str2 = end_match.group(2)\n",
    "                weekdays = {\n",
    "                 '월요일': 0,\n",
    "                '화요일': 1,\n",
    "                '수요일': 2,\n",
    "                '목요일': 3,\n",
    "                '금요일': 4,\n",
    "                '토요일': 5,\n",
    "                '일요일': 6\n",
    "                }\n",
    "            \n",
    "                target_weekday2 = weekdays[weekday_str2]\n",
    "                current_weekday2 = today.weekday()\n",
    "                days_to_target2 = (target_weekday2 - current_weekday2) % 7\n",
    "                date = datetime.now() + timedelta(days=days_to_target2)\n",
    "                fyear = date.year\n",
    "                fmonth = date.month\n",
    "                fday = date.day\n",
    "            elif end_match.group(1) and '년' in end_match.group(1):\n",
    "                fyear = int(end_match.group(1).strip(\"년\"))\n",
    "            else:\n",
    "                fyear = datetime.now().year\n",
    "                 \n",
    "            if (ftime_period == '오후' or ftime_period == '저녁' or ftime_period == '밤' or ftime_period == '낮') and int(fhour) < 12:\n",
    "                fhour = int(fhour) + 12\n",
    "                \n",
    "            \n",
    "            if end_match.group(7) and '반' in end_match.group(7):\n",
    "                fminute = 30\n",
    "            elif end_match.group(7):\n",
    "                fminute = int(end_match.group(7).strip(\"분\"))\n",
    "            else:\n",
    "                fminute = 0\n",
    "\n",
    "                \n",
    "        \n",
    "        else:\n",
    "            fyear, fmonth, fday, ftime_period, fhour, fminute = syear, smonth, sday, stime_period, shour, sminute\n",
    "\n",
    "        return syear, smonth, sday, stime_period, shour, sminute, fyear, fmonth, fday,ftime_period,  fhour, fminute,event\n",
    "    else:\n",
    "        return None, None, None, None, None, None, None,None, None, None, None, None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90d9d946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2024, 6, 25, None, 10, 30, 2024, 6, 25, None, 10, 30, '컴퓨터써야돼')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer=ner_inference3('내일 10시 반에 컴퓨터 써야돼')\n",
    "extract_schedule_info(answer[0],answer[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473606e",
   "metadata": {},
   "source": [
    "### 정밀도 (Precision): 모델이 양성으로 예측한 샘플 중에서 실제로 양성인 샘플의 비율을 나타냅니다. 예측한 양성 중에서 얼마나 정확하게 예측했는지를 측정하는 지표입니다. 정밀도는 FP(False Positive)를 줄이는 것이 목표일 때 중요합니다.\n",
    "\n",
    "### 재현율 (Recall): 실제 양성인 샘플 중에서 모델이 양성으로 예측한 샘플의 비율을 나타냅니다. 실제 양성 중에서 얼마나 많은 샘플을 찾아내는지를 측정하는 지표입니다. 재현율은 FN(False Negative)를 줄이는 것이 목표일 때 중요합니다.\n",
    "\n",
    "### F1 점수 (F1 Score): 정밀도와 재현율의 조화 평균으로, 이 둘의 균형을 평가하는 지표입니다. 정밀도와 재현율 사이의 균형을 나타내며, 이 둘 중 하나만 고려하는 것보다 더 종합적인 성능 평가를 제공합니다.\n",
    "\n",
    "### 혼동 행렬 (Confusion Matrix): 이는 모델이 어떻게 분류했는지를 보여주는 표입니다. 주로 이진 또는 다중 분류 모델의 결과를 시각화할 때 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185502b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803f071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a0e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce01df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8288f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11343179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
