{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPU62GxeVfjHRInbyNEwTew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62c7fc969ff94ceeb207fa1a774fe704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37b4cce670a3451f925d85855b98da34",
              "IPY_MODEL_f795ce9cf4d941849e0107b880c805ff",
              "IPY_MODEL_382250403db747e887aff9e3ea139726"
            ],
            "layout": "IPY_MODEL_d7e29746b2e948d3b7a9631450f0dc9d"
          }
        },
        "37b4cce670a3451f925d85855b98da34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10b5dea1f71d484fabedcfc037b428aa",
            "placeholder": "​",
            "style": "IPY_MODEL_a38080c6e95e436aae274a2e73f3454d",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "f795ce9cf4d941849e0107b880c805ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03d16d53e3fe420289ba7b5b15b29abc",
            "max": 511193709,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f6f0ae241434fc6b5f07a6bba059716",
            "value": 511193709
          }
        },
        "382250403db747e887aff9e3ea139726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d822a6132cd34106bb3d1a5d7b550ec6",
            "placeholder": "​",
            "style": "IPY_MODEL_9c053d9d4c284811b5f2c4e793f519f3",
            "value": " 511M/511M [00:06&lt;00:00, 82.6MB/s]"
          }
        },
        "d7e29746b2e948d3b7a9631450f0dc9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10b5dea1f71d484fabedcfc037b428aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a38080c6e95e436aae274a2e73f3454d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03d16d53e3fe420289ba7b5b15b29abc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f6f0ae241434fc6b5f07a6bba059716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d822a6132cd34106bb3d1a5d7b550ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c053d9d4c284811b5f2c4e793f519f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/520817/gp/blob/main/adhd(csv_%EC%B6%94%EA%B0%80%EB%A1%9C_%EB%A7%8C%EB%93%A4%EC%96%B4%EC%84%9C_%EB%AA%A8%EB%8D%B8_%EA%B5%AC%ED%98%84)_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3wuU86tE65c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "np.random.seed(42)\n",
        "cudnn.benchmark = False\n",
        "cudnn.deterministic = True\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "hHiShMlYFe58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "_3IXkvipR0mh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dbb2490-9e19-4c74-b971-6d661f043f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "id": "PL2JL6a_FfuX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5da02b0a-0195-4319-c068-020333ce9e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kmounlp/NER.git"
      ],
      "metadata": {
        "id": "MSGy3bujF0I3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e99c230a-d5aa-4fa1-93b3-9bf349f8886b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'NER' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob"
      ],
      "metadata": {
        "id": "x-T8lhgCNs-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list=[]"
      ],
      "metadata": {
        "id": "NglrP3LLNu1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in os.walk('NER/'):\n",
        "  for y in glob.glob(os.path.join(x[0], '*_NER.txt')):\n",
        "    file_list.append(y)"
      ],
      "metadata": {
        "id": "CO6Qof1eNxLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = sorted(file_list)"
      ],
      "metadata": {
        "id": "e9mgRtR0N6CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "file_path = file_list[0]\n",
        "file_path = Path(file_path)\n",
        "raw_text = file_path.read_text().strip()"
      ],
      "metadata": {
        "id": "ATeQEDq5OB0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install korpora\n",
        "\n",
        "from Korpora import Korpora\n",
        "corpus = Korpora.load(\"naver_changwon_ner\")"
      ],
      "metadata": {
        "id": "5rXoICJNO83V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3a6a84-f84a-44dd-a414-1ab5c6106eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: korpora in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.10/dist-packages (from korpora) (0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (4.66.2)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (2.31.0)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from korpora) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->korpora) (2024.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : 네이버 + 창원대\n",
            "    Repository : https://github.com/naver/nlp-challenge/tree/master/missions/ner\n",
            "    References : http://air.changwon.ac.kr/?page_id=10\n",
            "\n",
            "    개체명(Named Entity)은 인명, 기관명, 지명 등과 같이 문장 또는 문서에서 특정한 의미를 가지고 있는 단어 또는 어구를 말합니다.\n",
            "    이 때문에 개체명은 정보 검색 및 언어 이해를 위한 분석에서 주요한 대상으로 다루어지고 있습니다.\n",
            "    Data.ly에서는 개체명 코퍼스를 제공하여 연구에 도움을 드리고자 하며, 공개적인 리더보드를 통해 많은 분들의 연구 동향을 논의/공유하고자 합니다.\n",
            "    제공되는 코퍼스는 Data.ly에서 제작한 것으로, 연구 및 리더보드를 위한 학습으로 사용 가능하며 상업적인 목적으로 사용될 수 없습니다.\n",
            "\n",
            "    # License\n",
            "    연구 및 리더보드를 위한 학습으로 사용 가능하며 상업적인 목적으로 사용될 수 없습니다.\n",
            "\n",
            "[Korpora] Corpus `naver_changwon_ner` is already installed at /root/Korpora/naver_changwon_ner/train_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "옆에 파일 눌러서 ... 누른 후 파일 date.csv 업로드 따로 해야돼요!"
      ],
      "metadata": {
        "id": "ITLAolLDyjSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dat = pd.read_csv(\"/date.csv\", encoding = \"CP949\")"
      ],
      "metadata": {
        "id": "PULIOQxsOD6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dat_df = dat.iloc[:,0:1]"
      ],
      "metadata": {
        "id": "8XUIaJhEUkFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "네이버 데이터셋 전처리"
      ],
      "metadata": {
        "id": "9k8XkJDkrT3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naver_read_file(file_list):\n",
        "\n",
        "\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "\n",
        "    for doc in file_list:\n",
        "        tokens = []\n",
        "        tags = []\n",
        "        list1=doc.words\n",
        "        list2=doc.tags\n",
        "\n",
        "\n",
        "\n",
        "        for text,docs in zip(list1,list2):\n",
        "            try:\n",
        "                tag = docs\n",
        "                    # 2: pos, 3: ner\n",
        "                if tag in ['DAT_B', 'DAT_I']:\n",
        "                    if tag == 'DAT_B' or 'DAT_I':\n",
        "                        token = random.sample(dat_df['date'].tolist(), k=1)[0] #tag가 B-ORG이면 상장법인목록의 회사명 중 하나로 랜덤으로 대체\n",
        "                    else:\n",
        "                        token = text\n",
        "                else:\n",
        "                    token = text\n",
        "\n",
        "                for i, syllable in enumerate(token): # 음절 단위로 자르고\n",
        "                    tokens.append(syllable)\n",
        "                    if tag == '-':\n",
        "                          tag = 'O' # 태그가 '-'인 경우 'O'로 변경\n",
        "                    modi_tag = tag\n",
        "                    if i > 0:\n",
        "                        if tag[0] == 'B':\n",
        "                            modi_tag = 'I' + tag[1:]     # BIO tag를 부착\n",
        "\n",
        "                    tags.append(modi_tag)\n",
        "            except:\n",
        "                continue\n",
        "        token_docs.append(tokens)\n",
        "        tag_docs.append(tags)\n",
        "\n",
        "    return token_docs, tag_docs"
      ],
      "metadata": {
        "id": "0jp7_bpxPYvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naver_text,naver_tags =naver_read_file(corpus.train)"
      ],
      "metadata": {
        "id": "THjYXlx8e024"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(naver_text))\n",
        "print(len(naver_tags))"
      ],
      "metadata": {
        "id": "5V1r7hKMe06z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd756232-8653-41ea-84b2-d83d422af593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90000\n",
            "90000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(naver_text[4], end='\\n\\n') # 음절 단위로 잘 잘렸네요!\n",
        "print(naver_tags[4])"
      ],
      "metadata": {
        "id": "q9-iJwb1nNNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d15a499-c057-4cbc-eacb-21b12184370a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▲', '퍼', '거', '슨', '씨', '족', '의', '꾀']\n",
            "\n",
            "['O', 'PER_B', 'PER_B', 'PER_B', 'CVL_B', 'CVL_B', 'CVL_B', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "해양 뭐시기 데이터셋 전처리"
      ],
      "metadata": {
        "id": "_4L-7mfvrZEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def read_file(file_list):\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "    for file_path in file_list:\n",
        "        # print(\"read file from \", file_path)\n",
        "        file_path = Path(file_path)\n",
        "        raw_text = file_path.read_text().strip()\n",
        "        raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
        "        for doc in raw_docs:\n",
        "            tokens = []\n",
        "            tags = []\n",
        "            for line in doc.split('\\n'):\n",
        "                if line[0:1] == \"$\" or line[0:1] == \";\" or line[0:2] == \"##\":\n",
        "                    continue\n",
        "                try:\n",
        "                    tag = line.split('\\t')[3]# 2: pos, 3: ner\n",
        "\n",
        "                    if tag in ['B-DAT', 'I-DAT']:\n",
        "                        if tag == 'B-DAT':\n",
        "                            token = random.sample(dat_df['date'].tolist(), k=1)[0]\n",
        "                        elif tag == 'I-DAT':\n",
        "                            token = None\n",
        "                        else:\n",
        "                            token = line.split('\\t')[0]\n",
        "                    else:\n",
        "                        token = line.split('\\t')[0]\n",
        "\n",
        "                    for i, syllable in enumerate(token):    # 음절 단위로 잘라서\n",
        "                        tokens.append(syllable)\n",
        "                        modi_tag = tag\n",
        "                        if i > 0:\n",
        "                            if tag[0] == 'B':\n",
        "                                modi_tag = 'I' + tag[1:]    # BIO tag를 부착할게요 :-)\n",
        "                        tags.append(modi_tag)\n",
        "                except:\n",
        "                    continue\n",
        "            token_docs.append(tokens)\n",
        "            tag_docs.append(tags)\n",
        "\n",
        "    return token_docs, tag_docs"
      ],
      "metadata": {
        "id": "xOSLg1QNrfvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts, tags = read_file(file_list[:])"
      ],
      "metadata": {
        "id": "o724eldkrNL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts))\n",
        "print(len(tags))"
      ],
      "metadata": {
        "id": "fA1kWKcKtcCw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abced129-ad05-4907-a0e1-b4c43eaa3e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19263\n",
            "19263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[5], end='\\n\\n') # 음절 단위로 잘 잘렸네요!\n",
        "print(tags[5])"
      ],
      "metadata": {
        "id": "VDlObStTtcMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06ae861-b398-4025-f636-18509f815ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['첫', '_', '회', '를', '_', '시', '작', '으', '로', '_', '6', '0', '~', '7', '0', '년', '도', '까', '지', '_', '4', '일', '간', '_', '총', '_', '4', '회', '에', '_', '걸', '쳐', '_', '매', '_', '회', '_', '2', '편', '씩', '_', '총', '_', '8', '편', '이', '_', '공', '개', '될', '_', '예', '정', '이', '다', '.']\n",
            "\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DAT', 'I-DAT', 'I-DAT', 'I-DAT', 'I-DAT', 'I-DAT', 'I-DAT', 'O', 'O', 'O', 'B-DUR', 'I-DUR', 'I-DUR', 'O', 'O', 'O', 'B-NOH', 'I-NOH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NOH', 'I-NOH', 'O', 'O', 'O', 'O', 'B-NOH', 'I-NOH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "분리된 토큰을 하나의 개체명으로 묶어 주는 태깅 시스템을 위해 있는게 BIO, BIESO임.\n",
        "\n",
        "여러 개의 토큰으로 이루어진 개체명의 경우\n",
        "\n",
        "**개체명이 시작할 때** 'B,begin'\n",
        "\n",
        "**토큰이 개체명 중간에 있을 때** 'I,inside'\n",
        "\n",
        "**개체명의 마지막에 위치할 때** 'E,end'\n",
        "\n",
        "**하나의 토큰이 곧 하나의 개체명일 때** 'S,singleton'\n",
        "\n",
        "**토큰이 개체명이 아닐 경우** 'O,outside'"
      ],
      "metadata": {
        "id": "8w7_-eiNbz5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}\n",
        "\n",
        "for i, tag in enumerate(unique_tags):\n",
        "    print(tag)  #해양 tags 변경 전"
      ],
      "metadata": {
        "id": "SF0eRYxDkqwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f91e591-c59e-43b4-e8a7-505b8f1ff16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I-DAT\n",
            "B-PNT\n",
            "I-PNT\n",
            "I-ORG\n",
            "B-ORG\n",
            "B-TIM\n",
            "I-DUR\n",
            "B-DUR\n",
            "B-DAT\n",
            "B-NOH\n",
            "I-POH\n",
            "B-POH\n",
            "O\n",
            "I-MNY\n",
            "B-LOC\n",
            "I-PER\n",
            "I-LOC\n",
            "I-TIM\n",
            "B-MNY\n",
            "B-PER\n",
            "I-NOH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sublist in tags:\n",
        "    for i, label in enumerate(sublist):\n",
        "        if label == 'I-NOH':\n",
        "            sublist[i] = 'NOH_I'\n",
        "        elif label == 'I-PNT':\n",
        "            sublist[i] = 'PNT_I'\n",
        "        elif label == 'I-ORG':\n",
        "            sublist[i] = 'ORG_I'\n",
        "        elif label == 'I-DUR':\n",
        "            sublist[i] = 'DUR_I'\n",
        "        elif label == 'I-TIM':\n",
        "            sublist[i] = 'TIM_I'\n",
        "        elif label == 'I-PER':\n",
        "            sublist[i] = 'PER_I'\n",
        "        elif label == 'I-MNY':\n",
        "            sublist[i] = 'MNY_I'\n",
        "        elif label == 'B-MNY':\n",
        "            sublist[i] = 'MNY_B'\n",
        "        elif label == 'B-DAY':\n",
        "            sublist[i] = 'DAY_B'\n",
        "        elif label == 'B-TIM':\n",
        "            sublist[i] = 'TIM_B'\n",
        "        elif label == 'I-LOC':\n",
        "            sublist[i] = 'LOC_I'\n",
        "        elif label == 'B-PER':\n",
        "            sublist[i] = 'PER_B'\n",
        "        elif label == 'B-ORG':\n",
        "            sublist[i] = 'ORG_B'\n",
        "        elif label == 'B-LOC':\n",
        "            sublist[i] = 'LOC_B'\n",
        "        elif label == 'B-PNT':\n",
        "            sublist[i] = 'PNT_B'\n",
        "        elif label == 'B-NOH':\n",
        "            sublist[i] = 'NOH_B'\n",
        "        elif label == 'I-PER':\n",
        "            sublist[i] = 'PER_I'\n",
        "        elif label == 'B-LOC':\n",
        "            sublist[i] = 'LOC_B'\n",
        "        elif label == 'I-POH':\n",
        "            sublist[i] = 'POH_I'\n",
        "        elif label == 'I-DAT':\n",
        "            sublist[i] = 'DAT_I'\n",
        "        elif label == 'B-POH':\n",
        "            sublist[i] = 'POH_B'\n",
        "        elif label == 'B-DUR':\n",
        "            sublist[i] = 'DUR_B'\n",
        "        elif label == 'B-DAT':\n",
        "            sublist[i] = 'DAT_B'\n",
        "        elif label == '0':\n",
        "            sublist[i] = '0'"
      ],
      "metadata": {
        "id": "pw1se6PFj5sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}\n",
        "\n",
        "for i, tag in enumerate(unique_tags):\n",
        "    print(tag)  #해양 tags 변경 후"
      ],
      "metadata": {
        "id": "h7bFUznW56C_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1b63bb-8d66-49d3-8dc7-8527675144eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PER_B\n",
            "TIM_I\n",
            "POH_I\n",
            "DUR_B\n",
            "TIM_B\n",
            "POH_B\n",
            "LOC_B\n",
            "O\n",
            "PNT_B\n",
            "NOH_B\n",
            "MNY_B\n",
            "PNT_I\n",
            "LOC_I\n",
            "ORG_I\n",
            "DAT_I\n",
            "DUR_I\n",
            "NOH_I\n",
            "ORG_B\n",
            "DAT_B\n",
            "PER_I\n",
            "MNY_I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts.extend(naver_text)\n",
        "tags.extend(naver_tags)"
      ],
      "metadata": {
        "id": "0o94hYy66Fvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "metadata": {
        "id": "VHE_IE4O6jui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, tag in enumerate(unique_tags):\n",
        "    print(tag)  # 학습을 위한 label list를 확인합니다."
      ],
      "metadata": {
        "id": "zKpiYcZE6lW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51fb8c7-0302-4377-bc60-ea69d357ae36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PER_B\n",
            "TRM_B\n",
            "TIM_I\n",
            "POH_I\n",
            "AFW_I\n",
            "ANM_I\n",
            "PLT_I\n",
            "DUR_B\n",
            "FLD_B\n",
            "TIM_B\n",
            "POH_B\n",
            "EVT_B\n",
            "TRM_I\n",
            "ANM_B\n",
            "LOC_B\n",
            "EVT_I\n",
            "FLD_I\n",
            "O\n",
            "NUM_I\n",
            "AFW_B\n",
            "PNT_B\n",
            "NOH_B\n",
            "MNY_B\n",
            "MAT_I\n",
            "CVL_I\n",
            "PNT_I\n",
            "LOC_I\n",
            "ORG_I\n",
            "DAT_I\n",
            "DUR_I\n",
            "NOH_I\n",
            "PLT_B\n",
            "CVL_B\n",
            "NUM_B\n",
            "MAT_B\n",
            "ORG_B\n",
            "DAT_B\n",
            "PER_I\n",
            "MNY_I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for tag in list(tag2id.keys()) :\n",
        "    globals()[tag] = 0\n",
        "\n",
        "for tag in tags :\n",
        "    for ner in tag :\n",
        "        globals()[ner] += 1\n",
        "\n",
        "for tag in list(tag2id.keys()) :\n",
        "    print('{:>6} : {:>7,}'. format(tag, globals()[tag]))"
      ],
      "metadata": {
        "id": "dZKSVM847DXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c715b0-7fbc-41b9-e74c-a865afe729d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PER_B : 192,859\n",
            " TRM_B : 129,695\n",
            " TIM_I :   5,511\n",
            " POH_I :  37,156\n",
            " AFW_I :   9,433\n",
            " ANM_I :     172\n",
            " PLT_I :       5\n",
            " DUR_B :   1,207\n",
            " FLD_B :   9,715\n",
            " TIM_B :  10,831\n",
            " POH_B :   6,686\n",
            " EVT_B :  59,714\n",
            " TRM_I :  14,924\n",
            " ANM_B :  22,070\n",
            " LOC_B :  98,439\n",
            " EVT_I :  31,340\n",
            " FLD_I :     155\n",
            "     O : 3,067,701\n",
            " NUM_I :  26,913\n",
            " AFW_B :  23,535\n",
            " PNT_B :   1,672\n",
            " NOH_B :  11,051\n",
            " MNY_B :   1,440\n",
            " MAT_I :      62\n",
            " CVL_I :  15,293\n",
            " PNT_I :   4,613\n",
            " LOC_I :  17,533\n",
            " ORG_I :  68,227\n",
            " DAT_I :  66,187\n",
            " DUR_I :   4,573\n",
            " NOH_I :  23,967\n",
            " PLT_B :     952\n",
            " CVL_B : 266,517\n",
            " NUM_B : 228,990\n",
            " MAT_B :   1,001\n",
            " ORG_B : 255,988\n",
            " DAT_B : 143,060\n",
            " PER_I :  52,397\n",
            " MNY_I :   6,930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, test_texts, train_tags, test_tags = train_test_split(texts, tags, test_size=.2, random_state=42)"
      ],
      "metadata": {
        "id": "SDN7pFeX7ht2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train 문장 : {:>6,}' .format(len(train_texts)))\n",
        "print('Train 태그 : {:>6,}' .format(len(train_tags)))\n",
        "print('Test  문장 : {:>6,}' .format(len(test_texts)))\n",
        "print('Test  태그 : {:>6,}' .format(len(test_tags)))"
      ],
      "metadata": {
        "id": "q-YH9ZnT7hx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823da81e-bdfb-47d5-a50c-55709ce91ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 문장 : 87,410\n",
            "Train 태그 : 87,410\n",
            "Test  문장 : 21,853\n",
            "Test  태그 : 21,853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts[5]"
      ],
      "metadata": {
        "id": "TZOGxTHa7h1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aabaffd-2756-4846-8e97-0372a3864d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['첫',\n",
              " '출',\n",
              " '발',\n",
              " '남',\n",
              " '상',\n",
              " '은',\n",
              " '작',\n",
              " '은',\n",
              " '어',\n",
              " '간',\n",
              " '하',\n",
              " '나',\n",
              " '였',\n",
              " '어',\n",
              " '요',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT 토크나이저"
      ],
      "metadata": {
        "id": "rrTVAmV57umZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "MODEL_NAME = \"beomi/KcELECTRA-base-v2022\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "HWlZUwuB7h5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb46ae72-3785-4331-9f84-e6e7daf3394c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "metadata": {
        "id": "LgYgUvlX7z4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_token_id = tokenizer.pad_token_id # 0\n",
        "cls_token_id = tokenizer.cls_token_id # 101\n",
        "sep_token_id = tokenizer.sep_token_id # 102\n",
        "pad_token_label_id = tag2id['O']    # tag2id['O']\n",
        "cls_token_label_id = tag2id['O']\n",
        "sep_token_label_id = tag2id['O']"
      ],
      "metadata": {
        "id": "KyKDtxJK73Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_tokenizer(sent, max_seq_length):\n",
        "    pre_syllable = \"_\"\n",
        "    input_ids = [pad_token_id] * (max_seq_length - 1)\n",
        "    attention_mask = [0] * (max_seq_length - 1)\n",
        "    token_type_ids = [0] * max_seq_length\n",
        "    sent = sent[:max_seq_length-2]\n",
        "\n",
        "    for i, syllable in enumerate(sent):\n",
        "        if syllable == '_':\n",
        "            pre_syllable = syllable\n",
        "        if pre_syllable != \"_\":\n",
        "            syllable = '##' + syllable  # 중간 음절에는 모두 prefix를 붙입니다.\n",
        "            # 우리가 구성한 학습 데이터도 이렇게 구성되었기 때문이라고 함.\n",
        "            # 이순신은 조선 -> [이, ##순, ##신, ##은, 조, ##선]\n",
        "        pre_syllable = syllable\n",
        "\n",
        "        input_ids[i] = (tokenizer.convert_tokens_to_ids(syllable))\n",
        "        attention_mask[i] = 1\n",
        "\n",
        "    input_ids = [cls_token_id] + input_ids\n",
        "    input_ids[len(sent)+1] = sep_token_id\n",
        "    attention_mask = [1] + attention_mask\n",
        "    attention_mask[len(sent)+1] = 1\n",
        "    return {\"input_ids\":input_ids,\n",
        "            \"attention_mask\":attention_mask,\n",
        "            \"token_type_ids\":token_type_ids}"
      ],
      "metadata": {
        "id": "H56iODmK73I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ner_tokenizer(train_texts[0], 5))"
      ],
      "metadata": {
        "id": "uNzD1LHq78Sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e3a456-1695-4c14-ada8-121dcdd40e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [2, 1912, 4044, 4040, 3], 'attention_mask': [1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_sentences = []\n",
        "tokenized_test_sentences = []\n",
        "\n",
        "for text in train_texts:    # 전체 데이터를 tokenizing 합니다.\n",
        "    tokenized_train_sentences.append(ner_tokenizer(text, 128))\n",
        "for text in test_texts:\n",
        "    tokenized_test_sentences.append(ner_tokenizer(text, 128))"
      ],
      "metadata": {
        "id": "TvaNc5Nr78Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_tags(tags, max_seq_length):\n",
        "    # label 역시 입력 token과 개수를 맞춰줍니다\n",
        "    tags = tags[:max_seq_length-2]\n",
        "    labels = [tag2id[tag] for tag in tags]\n",
        "    labels = [tag2id['O']] + labels\n",
        "\n",
        "    padding_length = max_seq_length - len(labels)\n",
        "    labels = labels + ([pad_token_label_id] * padding_length)\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "VuyXg4Zo78bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2id"
      ],
      "metadata": {
        "id": "7jmQTT2X8B7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6086d79e-1fb3-4b92-b91b-51625900064c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'PER_B': 0,\n",
              " 'TRM_B': 1,\n",
              " 'TIM_I': 2,\n",
              " 'POH_I': 3,\n",
              " 'AFW_I': 4,\n",
              " 'ANM_I': 5,\n",
              " 'PLT_I': 6,\n",
              " 'DUR_B': 7,\n",
              " 'FLD_B': 8,\n",
              " 'TIM_B': 9,\n",
              " 'POH_B': 10,\n",
              " 'EVT_B': 11,\n",
              " 'TRM_I': 12,\n",
              " 'ANM_B': 13,\n",
              " 'LOC_B': 14,\n",
              " 'EVT_I': 15,\n",
              " 'FLD_I': 16,\n",
              " 'O': 17,\n",
              " 'NUM_I': 18,\n",
              " 'AFW_B': 19,\n",
              " 'PNT_B': 20,\n",
              " 'NOH_B': 21,\n",
              " 'MNY_B': 22,\n",
              " 'MAT_I': 23,\n",
              " 'CVL_I': 24,\n",
              " 'PNT_I': 25,\n",
              " 'LOC_I': 26,\n",
              " 'ORG_I': 27,\n",
              " 'DAT_I': 28,\n",
              " 'DUR_I': 29,\n",
              " 'NOH_I': 30,\n",
              " 'PLT_B': 31,\n",
              " 'CVL_B': 32,\n",
              " 'NUM_B': 33,\n",
              " 'MAT_B': 34,\n",
              " 'ORG_B': 35,\n",
              " 'DAT_B': 36,\n",
              " 'PER_I': 37,\n",
              " 'MNY_I': 38}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode_tags(train_tags[0], 5)"
      ],
      "metadata": {
        "id": "d2JC7k6e8CAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f9cb92c-204f-463f-db08-57e14f296a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17, 0, 0, 0, 17]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = []\n",
        "test_labels = []\n",
        "\n",
        "for tag in train_tags:\n",
        "    train_labels.append(encode_tags(tag, 128))\n",
        "\n",
        "for tag in test_tags:\n",
        "    test_labels.append(encode_tags(tag, 128))"
      ],
      "metadata": {
        "id": "G0u4iCxr8CEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_labels), len(test_labels)"
      ],
      "metadata": {
        "id": "PzyG7_6v8CJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8939215-9f4e-48bb-b386-ec88b94d5291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(87410, 21853)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token 데이터셋"
      ],
      "metadata": {
        "id": "UsmqYe7y8N92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class TokenDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = TokenDataset(tokenized_train_sentences, train_labels)\n",
        "test_dataset = TokenDataset(tokenized_test_sentences, test_labels)"
      ],
      "metadata": {
        "id": "9VO4H2NC8CNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade pip setuptools wheel\n",
        "#!pip install —-upgrade tokenizers\n",
        "#!pip install —-upgrade Cython\n",
        "#!pip install accelerate -U\n",
        "#!pip install transformers==4.3.0"
      ],
      "metadata": {
        "id": "AtU_7rnlt9GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JGa22OVp-kX",
        "outputId": "148de8d5-ce76-4cb4-c759-2e23e8291da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification, Trainer, TrainingArguments, AutoModelForTokenClassification,EarlyStoppingCallback\n",
        "import sys\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=5,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=1000, # 1000번쨰 steps마다 log를 보여줌\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=5,\n",
        "    save_strategy='steps', # steps로 해야 earlystop이 가능\n",
        "    evaluation_strategy='steps',\n",
        "    save_steps=1000, # 1000번쨰 step마다 저장\n",
        "    eval_steps=1000, # 1000번째 step마다 평가\n",
        "    seed=15,\n",
        "    load_best_model_at_end=True # 가장 좋은 성능의 모델로...\n",
        ")"
      ],
      "metadata": {
        "id": "tN6IMVxC8Q-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BertForTokenClassification"
      ],
      "metadata": {
        "id": "2REJe1Qdu_BP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(unique_tags))\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,# evaluation dataset\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)] #loss가 2번 감소하지 않으면 스탑\n",
        ")"
      ],
      "metadata": {
        "id": "p_R6K6En8RCS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "62c7fc969ff94ceeb207fa1a774fe704",
            "37b4cce670a3451f925d85855b98da34",
            "f795ce9cf4d941849e0107b880c805ff",
            "382250403db747e887aff9e3ea139726",
            "d7e29746b2e948d3b7a9631450f0dc9d",
            "10b5dea1f71d484fabedcfc037b428aa",
            "a38080c6e95e436aae274a2e73f3454d",
            "03d16d53e3fe420289ba7b5b15b29abc",
            "4f6f0ae241434fc6b5f07a6bba059716",
            "d822a6132cd34106bb3d1a5d7b550ec6",
            "9c053d9d4c284811b5f2c4e793f519f3"
          ]
        },
        "outputId": "9108d596-8432-4652-e0b8-49e5a2a4d7f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/511M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62c7fc969ff94ceeb207fa1a774fe704"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "boLMdjqRvFA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a9e1d2-ffeb-427b-e842-a5c8cf0e82d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "RBNcn9oQvFF5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "985bd08a-c080-477b-be05-c4ff4dd05081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "eJAU275jvFKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(test_dataset)\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)"
      ],
      "metadata": {
        "id": "S5IrcrcxEH2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = np.argmax(predictions.predictions, axis=-1)"
      ],
      "metadata": {
        "id": "p_RloNdCEH7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_ner = {i:j for j, i in tag2id.items()}\n",
        "f_label = [i for i, j in tag2id.items()]\n",
        "val_tags_l = [index_to_ner[x] for x in np.ravel(predictions.label_ids).astype(int).tolist()]\n",
        "y_predicted_l = [index_to_ner[x] for x in np.ravel(preds).astype(int).tolist()]"
      ],
      "metadata": {
        "id": "Exrej6TWEH9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report"
      ],
      "metadata": {
        "id": "yj6taY6AEH_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋 추가\n",
        "print(classification_report(val_tags_l, y_predicted_l, labels=f_label))"
      ],
      "metadata": {
        "id": "NIBjHBBG7z8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('kcelectra_base_new')"
      ],
      "metadata": {
        "id": "IeGUY8DaJp9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델이 저장되면 옆에 파일 누르면 content 파일에 모델이 저장될 거예요. 커서를 우측으로 갖다대면 점 세개가 나오는데 거기서 다운로드를 받아야해요. config든 뭐든 다 다운로드 받아야하고 시간 걸릴 거예요."
      ],
      "metadata": {
        "id": "LxBDVEvzysnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tag2id.values())"
      ],
      "metadata": {
        "id": "S71sM9HcLlEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장한 모델 불러오기\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "MODEL_NAME = \"beomi/KcELECTRA-base-v2022\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "tag2id = {'POH_B': 0, 'CVL_B': 1, 'FLD_B': 2, 'FLD_I': 3, 'EVT_I': 4,'AFW_B': 5,'CVL_I': 6,'NUM_B': 7,'MAT_I': 8, 'TIM_I': 9,'ANM_I': 10,'PER_I': 11,'POH_I': 12,\n",
        " 'TRM_I': 13,\n",
        " 'TIM_B': 14,\n",
        " 'ANM_B': 15,\n",
        " 'O': 16,\n",
        " 'DAT_I': 17,\n",
        " 'DUR_I': 18,\n",
        " 'PNT_B': 19,\n",
        " 'PNT_I': 20,\n",
        " 'MNY_I': 21,\n",
        " 'EVT_B': 22,\n",
        " 'ORG_I': 23,\n",
        " 'MNY_B': 24,\n",
        " 'ORG_B': 25,\n",
        " 'LOC_B': 26,\n",
        " 'PLT_I': 27,\n",
        " 'MAT_B': 28,\n",
        " 'DAT_B': 29,\n",
        " 'NUM_I': 30,\n",
        " 'AFW_I': 31,\n",
        " 'PLT_B': 32,\n",
        " 'DUR_B': 33,\n",
        " 'PER_B': 34,\n",
        " 'LOC_I': 35,\n",
        " 'NOH_B': 36,\n",
        " 'TRM_B': 37,\n",
        " 'NOH_I': 38}\n",
        "unique_tags={'POH_B', 'CVL_B', 'FLD_B', 'FLD_I', 'EVT_I', 'AFW_B', 'CVL_I', 'NUM_B', 'MAT_I', 'TIM_I', 'ANM_I', 'PER_I', 'POH_I', 'TRM_I', 'TIM_B', 'ANM_B', 'O', 'DAT_I', 'DUR_I', 'PNT_B', 'PNT_I', 'MNY_I', 'EVT_B', 'ORG_I', 'MNY_B', 'ORG_B', 'LOC_B', 'PLT_I', 'MAT_B', 'DAT_B', 'NUM_I', 'AFW_I', 'PLT_B', 'DUR_B', 'PER_B', 'LOC_I', 'NOH_B', 'TRM_B', 'NOH_I'}\n",
        "id2tag={0: 'POH_B',\n",
        " 1: 'CVL_B',\n",
        " 2: 'FLD_B',\n",
        " 3: 'FLD_I',\n",
        " 4: 'EVT_I',\n",
        " 5: 'AFW_B',\n",
        " 6: 'CVL_I',\n",
        " 7: 'NUM_B',\n",
        " 8: 'MAT_I',\n",
        " 9: 'TIM_I',\n",
        " 10: 'ANM_I',\n",
        " 11: 'PER_I',\n",
        " 12: 'POH_I',\n",
        " 13: 'TRM_I',\n",
        " 14: 'TIM_B',\n",
        " 15: 'ANM_B',\n",
        " 16: 'O',\n",
        " 17: 'DAT_I',\n",
        " 18: 'DUR_I',\n",
        " 19: 'PNT_B',\n",
        " 20: 'PNT_I',\n",
        " 21: 'MNY_I',\n",
        " 22: 'EVT_B',\n",
        " 23: 'ORG_I',\n",
        " 24: 'MNY_B',\n",
        " 25: 'ORG_B',\n",
        " 26: 'LOC_B',\n",
        " 27: 'PLT_I',\n",
        " 28: 'MAT_B',\n",
        " 29: 'DAT_B',\n",
        " 30: 'NUM_I',\n",
        " 31: 'AFW_I',\n",
        " 32: 'PLT_B',\n",
        " 33: 'DUR_B',\n",
        " 34: 'PER_B',\n",
        " 35: 'LOC_I',\n",
        " 36: 'NOH_B',\n",
        " 37: 'TRM_B',\n",
        " 38: 'NOH_I'}\n",
        "pad_token_id = tokenizer.pad_token_id # 0\n",
        "cls_token_id = tokenizer.cls_token_id # 101\n",
        "sep_token_id = tokenizer.sep_token_id # 102\n",
        "pad_token_label_id = tag2id['O']    # tag2id['O']\n",
        "cls_token_label_id = tag2id['O']\n",
        "sep_token_label_id = tag2id['O']\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained('kcelectra_base_new', num_labels=len(unique_tags))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "fiw4e8yPJsZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 토크나이저는 wordPiece tokenizer로 tokenizing 결과를 반환합니다.\n",
        "# 데이터 단위를 음절 단위로 변경했기 때문에, tokenizer도 음절 tokenizer로 변경\n",
        "\n",
        "# berttokenizer를 사용하는데 한국어 vocab이 8000개 정도 밖에 없고 그 안의 한국어들의 거의 음절로 존재\n",
        "# -> 음절 단위 tokenizer를 적용하면 vocab id를 어느 정도 획득할 수 있어 UNK가 별로 없을듯 하다\n",
        "def ner_tokenizer(sent, max_seq_length):\n",
        "    pre_syllable = \"_\"\n",
        "    input_ids = [pad_token_id] * (max_seq_length - 1)\n",
        "    attention_mask = [0] * (max_seq_length - 1)\n",
        "    token_type_ids = [0] * max_seq_length\n",
        "    sent = sent[:max_seq_length-2]\n",
        "\n",
        "    for i, syllable in enumerate(sent):\n",
        "        if syllable == '_':\n",
        "            pre_syllable = syllable\n",
        "        if pre_syllable != \"_\":\n",
        "            syllable = '##' + syllable  # 중간 음절에는 모두 prefix를 붙입니다.\n",
        "            # 우리가 구성한 학습 데이터도 이렇게 구성되었기 때문이라고 함.\n",
        "            # 이순신은 조선 -> [이, ##순, ##신, ##은, 조, ##선]\n",
        "        pre_syllable = syllable\n",
        "\n",
        "        input_ids[i] = (tokenizer.convert_tokens_to_ids(syllable))\n",
        "        attention_mask[i] = 1\n",
        "\n",
        "    input_ids = [cls_token_id] + input_ids\n",
        "    input_ids[len(sent)+1] = sep_token_id\n",
        "    attention_mask = [1] + attention_mask\n",
        "    attention_mask[len(sent)+1] = 1\n",
        "    return {\"input_ids\":input_ids,\n",
        "            \"attention_mask\":attention_mask,\n",
        "            \"token_type_ids\":token_type_ids}"
      ],
      "metadata": {
        "id": "S_lIHQTeJsWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_inference(text) :\n",
        "\n",
        "    model.eval()\n",
        "    text = text.replace(' ', '_')\n",
        "\n",
        "    predictions , true_labels = [], []\n",
        "\n",
        "    tokenized_sent = ner_tokenizer(text, len(text)+2)\n",
        "    input_ids = torch.tensor(tokenized_sent['input_ids']).unsqueeze(0).to(device)\n",
        "    attention_mask = torch.tensor(tokenized_sent['attention_mask']).unsqueeze(0).to(device)\n",
        "    token_type_ids = torch.tensor(tokenized_sent['token_type_ids']).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids)\n",
        "\n",
        "    logits = outputs['logits']\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = token_type_ids.cpu().numpy()\n",
        "\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "    pred_tags = [list(tag2id.keys())[p_i] for p in predictions for p_i in p]\n",
        "\n",
        "    print('{}\\t{}'.format(\"TOKEN\", \"TAG\"))\n",
        "    print(\"===========\")\n",
        "    # for token, tag in zip(tokenizer.decode(tokenized_sent['input_ids']), pred_tags):\n",
        "    #   print(\"{:^5}\\t{:^5}\".format(token, tag))\n",
        "    for i, tag in enumerate(pred_tags):\n",
        "        print(\"{:^5}\\t{:^5}\".format(tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'][i]), tag))"
      ],
      "metadata": {
        "id": "2Zl7-l9DMZ8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text='모레 10시 반 안과 방문 예정, 시력 물어보기'"
      ],
      "metadata": {
        "id": "BJ9W6DNiMhRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_inference(text)"
      ],
      "metadata": {
        "id": "l7se1iUiJsUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rg_xQwCBJsQD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}